###########################################################################################
############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1

# The port the socket server listens on
port=9092

listeners=PLAINTEXT://:9092
host.name=ip


############################# Socket Server Settings #############################

# The number of threads handling network requests
num.network.threads=32

# The number of threads doing disk I/O
num.io.threads=64

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600

# This sets the queue size that holds the pending messages while others are being processed by the IO threads.
queued.max.requests=500

# Enable auto creation of topic on the server. 
# If this is set to true then attempts to produce data or fetch metadata for a non-existent topic will automatically create it with the default replication factor and number of partitions.
auto.create.topics.enable=false



############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs=/data/kafka-data/1

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=4

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

# The maximum size in bytes we allow for the offset index for each log segment. 
# Note that we will always pre-allocate a sparse file with this much space and shrink it down when the log rolls. 
# If the index fills up we will roll a new log segment even if we haven’t reached the log.segment.bytes limit. 
# This setting can be overridden on a per-topic basis
log.index.size.max.bytes=10485760

#	The byte interval at which we add an entry to the offset index. 
# When executing a fetch request the server must do a linear scan for up to this many bytes to find the correct position in the log to begin and end the fetch.
# So setting this value to be larger will mean larger index files (and a bit more memory usage) but less scanning. 
log.index.interval.bytes=4096

# This is largest message size Kafka will allow to be appended to this topic. 
# Note that if you increase this size you must also increase your consumer’s fetch size so they can fetch such large messages.
message.max.bytes=1000000

offset.metadata.max.bytes = 1024

############################# Log Flush Policy #############################
default.replication.factor=3
log.flush.interval.messages=100000
log.flush.interval.ms=50000
log.flush.scheduler.interval.ms=2000
log.cleanup.policy = delete
############################# Log Retention Policy #############################
log.retention.hours=24
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
log.cleaner.enable=true
log.roll.hours=168

############################# Zookeeper #############################

zookeeper.connect=ip:port/path
zookeeper.connection.timeout.ms=6000
zk.sync.time.ms=2000

############################# no data loss ###########################
default.replication.factor=3
min.insync.replicas=2
unclean.leader.election.enable=false

############################# Replication configurations ########################
num.replica.fetchers=4
replica.fetch.max.bytes=1048576
replica.fetch.wait.max.ms=500

# The frequency with which each replica saves its high watermark to disk to handle recovery.
replica.high.watermark.checkpoint.interval.ms=5000

# The socket timeout for network requests to the leader for replicating data.
replica.socket.timeout.ms=30000

# The socket receive buffer for network requests to the leader for replicating data.
replica.socket.receive.buffer.bytes=65536

# If a follower hasn’t sent any fetch requests for this window of time, the leader will remove the follower from ISR (in-sync replicas) and treat it as dead.  
replica.lag.time.max.ms=10000

# If a replica falls more than this many messages behind the leader, the leader will remove the follower from ISR and treat it as dead.
replica.lag.max.messages=4000

controller.socket.timeout.ms=30000
controller.message.queue.size=10

# Graceful shutdown
# When a server is stopped gracefully it has two optimizations it will take advantage of:
# It will sync all its logs to disk to avoid needing to do any log recovery when it restarts (i.e. validating the checksum for all messages in the tail of the log). Log recovery takes time so this speeds up intentional restarts.
# It will migrate any partitions the server is the leader for to other replicas prior to shutting down. This will make the leadership transfer faster and minimize the time each partition is unavailable to a few milliseconds.
# Syncing the logs will happen automatically whenever the server is stopped other than by a hard kill, but the controlled leadership migration requires using a special setting:
controlled.shutdown.enable = true
controlled.shutdown.max.retries = 3
controlled.shutdown.retry.backoff.ms = 5000


############# Balancing leadership ##############

# Whenever a broker stops or crashes leadership for that broker's partitions transfers to other replicas. This means that by default when the broker is restarted it will only be a follower for all its partitions, meaning it will not be used for client reads and writes.
auto.leader.rebalance.enable = false
leader.imbalance.per.broker.percentage = 10
leader.imbalance.check.interval.seconds = 300

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=1
